# ğŸ“‚ Scalable Image Import System (Google Drive â†’ S3)

This repository contains a **multi-service backend** that bulk-imports images from a **public Google Drive folder URL** to **AWS S3** and stores their metadata in **MySQL**.  It exposes two HTTP endpoints:

| Method | Path | Description |
|-------|------|-------------|
| `POST` | `/import/google-drive` | Import all images from a public Drive folder |
| `GET`  | `/images`             | List previously imported images |

---

## ğŸ–‡ï¸ Architecture

```
 FastAPI API (gateway)  â”€â”¬â”€>  Celery worker(s)  â”€â”¬â”€>  AWS S3  (images)
                        â”‚                      â””â”€>  MySQL   (metadata)
                        â””â”€>  RabbitMQ (task queue)
```

Each box runs in its own Docker container and can be scaled independently (`docker compose up --scale worker=5`).  The design supports importing **10 000+ images** by streaming files and using idempotent database writes.

---

## ğŸš€ Quick-start (local)

> **Prerequisites**: Docker Desktop (or Podman) installed and running.

1. **Clone the repo**

   ```bash
   git clone https://github.com/your-handle/image-import-system.git
   cd image-import-system
   ```

2. **Create an `.env` file**

   ```bash
   cp .env.sample .env
   # edit .env and provide real AWS credentials & bucket name
   ```

3. **Build and start the stack**

   ```bash
   docker compose up --build -d
   ```

4. **Run one-time DB migration** (creates the `images` table)

   ```bash
   docker compose run --rm gateway      python -c "import asyncio,common.db as d, common.models; asyncio.run(d.Base.metadata.create_all(d.engine))"
   ```

5. **Import a Google Drive folder**

   ```bash
   curl -X POST http://localhost:8000/import/google-drive         -H "Content-Type: application/json"         -d '{"folder_url": "https://drive.google.com/drive/folders/1YourFolderID"}'
   # => {"task_id": "c0123..."}
   ```

6. **Retrieve imported images**

   ```bash
   curl http://localhost:8000/images | jq
   ```

---

## ğŸ› ï¸ Project layout

```
.
â”œâ”€â”€ docker-compose.yml   # orchestrates all services
â”œâ”€â”€ .env.sample          # copy â†’ .env and fill secrets
â”œâ”€â”€ common/              # shared SQLAlchemy models / DB engine
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db.py
â”‚   â””â”€â”€ models.py
â”œâ”€â”€ gateway/             # FastAPI HTTP service
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ main.py
â””â”€â”€ worker/              # Celery background importer
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ google.py        # Google Drive helper
    â”œâ”€â”€ storage.py       # S3 uploader
    â””â”€â”€ tasks.py         # Celery task definition
```

---

## ğŸ“ Detailed explanation

### 1. Gateway service (`/gateway`)

* **Framework**: FastAPI
* **Endpoints**:
  * `POST /import/google-drive` â€“ Extracts the Drive *folder ID* from the supplied URL and enqueues a Celery task.
  * `GET /images` â€“ Returns all rows from the `images` table.
* **Environment**: Receives DB URL, Celery broker URL and AWS creds via variables.

### 2. Worker service (`/worker`)

* **Celery** worker listening on RabbitMQ.
* **`google.py`** uses the _gdown_ library to walk a public Drive folder and download images to a temporary directory.
* Each file is **stream-uploaded** to S3 via Boto3.  Metadata is recorded with `INSERT â€¦ ON DUPLICATE KEY IGNORE` semantics for idempotency.
* Retries: Celery `acks_late=True` and `max_retries=3` provide at-least-once processing.

### 3. Database (`/common`)

* MySQL 8 with a single table `images` (`google_drive_id` primary key).
* Async SQLAlchemy engine is shared by both services (`common/db.py`).

---

## ğŸ—ï¸ Production notes

| Concern | Approach |
|---------|----------|
| **Scalability** | Scale `worker` replicas; DB and S3 are fully managed services |
| **Fault tolerance** | Tasks acknowledge after success; failed attempts are retried |
| **Idempotency** | Primary key on `google_drive_id` prevents duplicates |
| **Observability** | Logs via Docker; add Prometheus/Grafana in real deployments |

---

## âœ¨ Extending the project

* **Dropbox support** â€“ Implement a similar iterator in `worker/dropbox.py` and register another Celery task.
* **Filtering** â€“ Add query params to `GET /images` and extend the SQLAlchemy query.
* **Deployment** â€“ Adapt `docker-compose.yml` to k8s manifests or ECS task defs.

---

## ğŸ”‘ License

MIT
